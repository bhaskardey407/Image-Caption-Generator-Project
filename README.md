# Image Caption Generator Project

## Introduction
This repository presents the implementation of an Image Caption Generator using Deep Learning techniques. The project utilizes the Flickr8k dataset sourced from Kaggle, containing images along with corresponding captions.

## Dataset
The dataset used for this project is the Flickr8k dataset, accessible through the following Kaggle link: [Flickr8k Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)

## Model Architecture
The Image Caption Generator is built using a combination of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks. Specifically, VGG16 is employed for feature extraction from the input images, while LSTM is utilized to decode the descriptions corresponding to the images.

## Stable Diffusers
In addition to the CNN-LSTM model, Stable Diffusers are utilized to enhance the accuracy of image generation based on the captions generated by the Image Caption Generator model. Stable Diffusers play a crucial role in refining the images generated from the textual descriptions, resulting in more accurate representations.

## Repository Structure
The repository comprises the following components:
- `code/`: Contains the source code for the Image Caption Generator model and Stable Diffusers.
- `data/`: Directory for storing the Flickr8k dataset.
- `results/`: Stores the output and results obtained from the model.
- `README.md`: Detailed information about the project, including setup instructions, usage guidelines, and results summary.

## Usage
To utilize the code and reproduce the results, follow these steps:
1. Clone the repository to your local machine.
2. Download the Flickr8k dataset from the provided Kaggle link and place it in the `data/` directory.
3. Navigate to the `code/` directory and run the appropriate scripts for model training, caption generation, and image generation.
4. Refer to the `README.md` file for detailed instructions on running the code and interpreting the results.

## Conclusion
The Image Caption Generator Project demonstrates the capability of Deep Learning models to generate textual descriptions for images, enabling applications in various domains such as image indexing, retrieval, and understanding. By combining CNNs for feature extraction and LSTMs for sequence generation, along with the utilization of Stable Diffusers for image refinement, the project achieves enhanced accuracy in generating captions and corresponding images.

For any inquiries or suggestions, please feel free to contact the project contributor.

## Contributors
- Bhaskar Dey
